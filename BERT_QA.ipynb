{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (3.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2020.2.20)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.8.1rc1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.48.0)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.24.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (20.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.10)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.1.85)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.14.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.6)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch) (1.18.5)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "#Note if you are using google colab - please go to Runtime -> Change runtime type  and select GPU as Hardware accelerator. This will make notebook run faster.\n",
    "#github link: https://github.com/sanigam/BERT_QA_Medium\n",
    "\n",
    "\n",
    "#Install following libraries before first run. For subsequent runs, you may comment these\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "\n",
    "#Import libraries\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading BERT model already fine-tuned on SQuAD Question Answer Dataset. This 1.3 GB download and may take sometime\n",
    "# Note that we are using uncased model so all answers will be in lower case\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting bert tokenizer\n",
    "tokenizer_for_bert = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 6.13, 6.94, 'washington')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bert_answering_machine ( question, passage, max_len =  512):\n",
    "    ''' Function to provide answer from passage for question asked.\n",
    "        This function takes question as well as the passage \n",
    "        It retuns answer from the passage, along with start/end token index for the answer and start/end token scores\n",
    "        The scores can be used to rank answers if we are searching answers for same question in multiple passages\n",
    "        Value of max_len can not exceed 512. If length of question + passage + special tokens is bigger than max_len, function will truncate extra portion.\n",
    "        \n",
    "    '''\n",
    "  \n",
    "    #Tokenize input question and passage. Keeping maximum number of tokens as specified by max_len parameter. This will also add special tokens - [CLS] and [SEP]\n",
    "    input_ids = tokenizer_for_bert.encode ( question, passage,  max_length= max_len, truncation=True)  \n",
    "    \n",
    "    \n",
    "    #Getting number of tokens in 1st sentence (question) and 2nd sentence (passage)\n",
    "    cls_index = input_ids.index(102) #Getting index of first CLS token\n",
    "    len_question = cls_index + 1       # length of question (1st sentence)\n",
    "    len_answer = len(input_ids)- len_question  # length of answer (2nd sentence)\n",
    "    \n",
    "    \n",
    "    #BERT need Segment Ids to understand which tokens belong to sentence 1 and which to sentence 2\n",
    "    segment_ids =  [0]*len_question + [1]*(len_answer)  #Segment ids will be 0 for question and 1 for answer\n",
    "    \n",
    "    #Converting token ids to tokens\n",
    "    tokens = tokenizer_for_bert.convert_ids_to_tokens(input_ids) \n",
    "    \n",
    "    \n",
    "    # getting start and end scores for answer. Converting input arrays to torch tensors before passing to the model\n",
    "    start_token_scores, end_token_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]) )\n",
    "\n",
    "    #Converting scores tensors to numpy arrays so that we can use numpy functions\n",
    "    start_token_scores = start_token_scores.detach().numpy().flatten()\n",
    "    end_token_scores = end_token_scores.detach().numpy().flatten()\n",
    "    \n",
    "    #Picking start index and end index of answer based on start/end indices with highest scores\n",
    "    answer_start_index = np.argmax(start_token_scores)\n",
    "    answer_end_index = np.argmax(end_token_scores)\n",
    "\n",
    "    #Getting scores for start token and end token of the answer. Also rounding it to 2 decimal digits\n",
    "    start_token_score = np.round(start_token_scores[answer_start_index], 2)\n",
    "    end_token_score = np.round(end_token_scores[answer_end_index], 2)\n",
    "    \n",
    "   \n",
    "    #Combining subwords starting with ## so that we can see full words in output. Note tokenizer breaks words which are not in its vocab.\n",
    "    answer = tokens[answer_start_index] #Answer starts with start index, we got based on highest score\n",
    "    for i in range(answer_start_index + 1, answer_end_index + 1):\n",
    "        if tokens[i][0:2] == '##':  # Token for a splitted word starts with ##\n",
    "            answer += tokens[i][2:] # If token start with ## we remove ## and combine it with previous word so as to restore the unsplitted word\n",
    "        else:\n",
    "            answer += ' ' + tokens[i]  # If token does not start with ## we just put a space in between while combining tokens\n",
    "            \n",
    "    # Few patterns indicating that BERT does not get answer from the passage for question asked\n",
    "    if ( answer_start_index == 0) or (start_token_score < 0 ) or  (answer == '[SEP]') or ( answer_end_index <  answer_start_index):\n",
    "        answer = \"Sorry!, I could not find  an answer in the passage.\"\n",
    "    \n",
    "    return ( answer_start_index, answer_end_index, start_token_score, end_token_score,  answer)\n",
    "\n",
    "\n",
    "#Testing function\n",
    "bert_answering_machine (\"Which state john's friend lives\", 'My name is John. I live in San Jose, California. Rob is my friend. He lives in Seattle, Washington')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage:\n",
      " John is a 10 years old boy. He is the son of Robert Smith.  Elizabeth Davis is Robert's wife. She teaches at UC Berkeley. Sophia Smith is Elizabeth's daughter. She studies at UC Davis\n",
      "Length of the passage: 34 words\n",
      "\n",
      "Question 1:\n",
      " Who is John's sister\n",
      "\n",
      "Answer from BERT:  sophia smith \n",
      "\n",
      "\n",
      "Question 2:\n",
      " Which college does John's sister attend\n",
      "\n",
      "Answer from BERT:  uc davis \n",
      "\n",
      "\n",
      "Question 3:\n",
      " Who is the president of UC Davis\n",
      "\n",
      "Answer from BERT:  Sorry!, I could not find  an answer in the passage. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BERT Question-Answer Sample 1\n",
    "passage=\"John is a 10 years old boy. He is the son of Robert Smith.  Elizabeth Davis is Robert's wife. She teaches at UC Berkeley. Sophia Smith is Elizabeth's daughter. She studies at UC Davis\"\n",
    "\n",
    "\n",
    "print('Passage:\\n', passage )\n",
    "print (f'Length of the passage: {len(passage.split())} words')\n",
    "\n",
    "question1 =\"Who is John's sister\" #BERT needs to apply some logic to answer this\n",
    "print ('\\nQuestion 1:\\n', question1)\n",
    "#Getting answer from BERT\n",
    "_, _ , _ , _, ans  = bert_answering_machine ( question1, passage)\n",
    "print('\\nAnswer from BERT: ', ans ,  '\\n')\n",
    "\n",
    "\n",
    "question2 =\"Which college does John's sister attend\"   #BERT needs to answer intermediate question (Question 1) to answer this \n",
    "print ('\\nQuestion 2:\\n', question2)\n",
    "#Getting answer from BERT\n",
    "_, _ , _ , _, ans  = bert_answering_machine ( question2, passage)\n",
    "print('\\nAnswer from BERT: ', ans ,  '\\n')\n",
    "\n",
    "question3 =\"Who is the president of UC Davis\" # BERT can not answer this from this passage\n",
    "print ('\\nQuestion 3:\\n', question3)\n",
    "#Getting answer from BERT\n",
    "_, _ , _ , _, ans  = bert_answering_machine ( question3, passage)\n",
    "print('\\nAnswer from BERT: ', ans ,  '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage:\n",
      "  Apple has told employees it'll provide them with paid time off to vote in the US presidential election on Nov. 3, according to a report. Workers, who wish to vote that Tuesday will be given up to four hours of pay, Bloomberg reported Friday citing an internal Apple memo. It follows Twitter in June making Election Day a paid holiday for US employees. For retail team members and hourly workers across the company, if you are scheduled to work this Election Day, we will be providing up to four hours of paid time off if you need it to get to the polls, said Deirdre O'Brien, Apple's senior vice president of retail and people, in the reported memo. Teams can also use this time to volunteer as an election worker at one of your local polling stations. Apple didn't immediately respond to a request for comment. Since Election Day in the US falls on a Tuesday, it can be difficult for people to find time outside of work to visit a polling place and vote \n",
      "Length of the passage: 175 words\n",
      "\n",
      "Question 1:\n",
      " On what date we have Election Day\n",
      "\n",
      "Answer from BERT:  nov . 3 \n",
      "\n",
      "\n",
      "Question 2:\n",
      " What's the concern discussed here\n",
      "\n",
      "Answer from BERT:  it can be difficult for people to find time outside of work to visit a polling place and vote \n",
      "\n",
      "\n",
      "Question 3:\n",
      " Who is Senior VP at Apple mentioned in this passage \n",
      "\n",
      "Answer from BERT:  deirdre o ' brien \n",
      "\n",
      "\n",
      "Question 4:\n",
      " How Apple is addressing the issue \n",
      "\n",
      "Answer from BERT:  apple has told employees it ' ll provide them with paid time off to vote in the us presidential election on nov . 3 \n",
      "\n",
      "\n",
      "Question 5:\n",
      " What's the alternate use of paid time off \n",
      "\n",
      "Answer from BERT:  to volunteer as an election worker at one of your local polling stations \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BERT Question-Answer Sample 2\n",
    "\n",
    "passage = \" Apple has told employees it'll provide them with paid time off to vote in the US presidential election on Nov. 3, according to a report. \\\n",
    "Workers, who wish to vote that Tuesday will be given up to four hours of pay, Bloomberg reported Friday citing an internal Apple memo. \\\n",
    "It follows Twitter in June making Election Day a paid holiday for US employees. For retail team members and hourly workers across the company, \\\n",
    "if you are scheduled to work this Election Day, we will be providing up to four hours of paid time off if you need it to get to the polls, \\\n",
    "said Deirdre O'Brien, Apple's senior vice president of retail and people, in the reported memo. Teams can also use this time to volunteer as an \\\n",
    "election worker at one of your local polling stations. Apple didn't immediately respond to a request for comment. \\\n",
    "Since Election Day in the US falls on a Tuesday, it can be difficult for people to find time outside of work to visit a polling place and vote \"\n",
    "\n",
    "print('Passage:\\n', passage )\n",
    "print (f'Length of the passage: {len(passage.split())} words')\n",
    "\n",
    "question1 =\"On what date we have Election Day\"\n",
    "print ('\\nQuestion 1:\\n', question1)\n",
    "#Getting answer from BERT\n",
    "_, _ , _ , _, ans  = bert_answering_machine ( question1, passage)\n",
    "print('\\nAnswer from BERT: ', ans ,  '\\n')\n",
    "\n",
    "question2 =\"What's the concern discussed here\"\n",
    "print ('\\nQuestion 2:\\n', question2)\n",
    "#Getting answer from BERT\n",
    "_, _ , _ , _, ans  = bert_answering_machine ( question2, passage)\n",
    "print('\\nAnswer from BERT: ', ans ,  '\\n')\n",
    "\n",
    "question3 =\"Who is Senior VP at Apple mentioned in this passage \"\n",
    "print ('\\nQuestion 3:\\n', question3)\n",
    "#Getting answer from BERT\n",
    "_, _ , _ , _, ans  = bert_answering_machine ( question3, passage)\n",
    "print('\\nAnswer from BERT: ', ans ,  '\\n')\n",
    "\n",
    "\n",
    "question4 =\"How Apple is addressing the issue \"\n",
    "print ('\\nQuestion 4:\\n', question4)\n",
    "#Getting answer from BERT\n",
    "_, _ , _ , _, ans  = bert_answering_machine ( question4, passage)\n",
    "print('\\nAnswer from BERT: ', ans ,  '\\n')\n",
    "\n",
    "question5 =\"What's the alternate use of paid time off \"\n",
    "print ('\\nQuestion 5:\\n', question5)\n",
    "#Getting answer from BERT\n",
    "_, _ , _ , _, ans  = bert_answering_machine ( question5, passage)\n",
    "print('\\nAnswer from BERT: ', ans ,  '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage:\n",
      " BERT, which is an acronym for Bi-directional Encoder Representation from Transformer, is a state of the art language model which can be used for various natural language processing (NLP) tasks.  My objective is to introduce BERT at a high level, and enable you to create practical applications using BERT. You need to have basic knowledge of Python as well as a basic idea of machine learning. After going through this post, you should be able to use BERT for Binary or multiclass classification or Regression model or Question Answering Application. BERT brings the transfer learning paradigm into the natural language processing area. Transfer learning means a model developed for a task can be reused as a starting point for another task. BERT is trained on the entirety of Wikipedia text (~2.5 billion words) as well as a book corpus (800 million words). You don’t need to repeat this compute intensive process to make practical use of BERT.For specific tasks such as classification or question answering you just need to do incremental training on a much smaller dataset. This process is called fine tuning. This is like getting a person who is proficient in English and providing them extra guidance on how to identify positive sentiment vs negative sentiment from movie reviews.This is a quick introduction on the BERT pre-training process. For practical uses, you will get a BERT pretrained model and you do not need to perform this step. BERT takes 2 chunks of text, which may include multiple sentences, as inputs (shown in the diagram above). These 2 chunks are referred to as Sentence 1 and Sentence 2 in the diagram above.  Before feeding sentences to BERT, 15% of words are masked. Also, sentence 2 may not always be in continuation of sentence 1. BERT’s pre-training process is like teaching the English language to the BERT model so that it can be used for various tasks which need knowledge of English. This is accomplished by 2 practice tasks given to BERT: 1) Predict masked (hidden) tokens: To illustrate, words “favorite” and “to” are masked in the diagram. BERT will try to predict these masked tokens as part of pre-training. This is similar to the “fill in the blanks” task we may give to a student learning English. While trying to fill the missing words, the student will learn the language. This is referred to as Masked Language Model (MLM) in the natural language processing area. 2) Next sentence prediction: Along with the technique mentioned above, BERT tries to predict if sentence 2 comes right after sentence 1 or not. This provides deeper understanding on sentence dependencies. To use BERT for practical use, we need to fine-tune it for specific tasks. This process finetunes the pre-trained model so that it can perform specific tasks such as text classification, sentiment analysis, question answering. In this process,BERT parameters get adjusted to do the specific task. \n",
      "Length of the passage: 483 words\n",
      "\n",
      "Question 1:\n",
      " What is full form of BERT\n",
      "\n",
      "Answer from BERT:  bi - directional encoder representation from transformer \n",
      "\n",
      "\n",
      "Question 2:\n",
      " What is author's purpose in writing this article \n",
      "\n",
      "Answer from BERT:  to introduce bert at a high level , and enable you to create practical applications using bert \n",
      "\n",
      "\n",
      "Question 3:\n",
      " What is full form of NLP \n",
      "\n",
      "Answer from BERT:  natural language processing \n",
      "\n",
      "\n",
      "Question 4:\n",
      " What is Transfer Learning \n",
      "\n",
      "Answer from BERT:  a model developed for a task can be reused as a starting point for another task \n",
      "\n",
      "\n",
      "Question 5:\n",
      " What is pre-training \n",
      "\n",
      "Answer from BERT:  teaching the english language to the bert model so that it can be used for various tasks which need knowledge of english \n",
      "\n",
      "\n",
      "Question 6:\n",
      " What corpus BERT was pre-trained \n",
      "\n",
      "Answer from BERT:  bert is trained on the entirety of wikipedia text ( ~ 2 . 5 billion words ) as well as a book corpus ( 800 million words ) \n",
      "\n",
      "\n",
      "Question 7:\n",
      " Do we need to pre-train BERT model for general practical applications \n",
      "\n",
      "Answer from BERT:  you do not need to perform this step \n",
      "\n",
      "\n",
      "Question 8:\n",
      " What is fine-tuning \n",
      "\n",
      "Answer from BERT:  for specific tasks such as classification or question answering you just need to do incremental training on a much smaller dataset \n",
      "\n",
      "\n",
      "Question 9:\n",
      " What is MLM \n",
      "\n",
      "Answer from BERT:  masked language model \n",
      "\n",
      "\n",
      "Question 10:\n",
      " Which words are masked in the diagram \n",
      "\n",
      "Answer from BERT:  “ favorite ” and “ to ” \n",
      "\n",
      "\n",
      "Question 11:\n",
      " What perentage of words are hidden when feeding text to BERT for pre-trainining \n",
      "\n",
      "Answer from BERT:  15 % of words are masked \n",
      "\n",
      "\n",
      "Question 12:\n",
      " What is the analogy provided by the author, for masked language modelling used in BERT pre-training \n",
      "\n",
      "Answer from BERT:  “ fill in the blanks ” task we may give to a student learning english \n",
      "\n",
      "\n",
      "Question 13:\n",
      " What is Random Forest alogorithm \n",
      "\n",
      "Answer from BERT:  Sorry!, I could not find  an answer in the passage. \n",
      "\n",
      "\n",
      "Question 14:\n",
      " How this post may help you \n",
      "\n",
      "Answer from BERT:  you should be able to use bert for binary or multiclass classification or regression model or question answering application \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let BERT read a version of my medium article and answer  some tecnical questions!\n",
    "# BERT Question-Answer Sample 3\n",
    "\n",
    "passage= 'BERT, which is an acronym for Bi-directional Encoder Representation from Transformer, is a state of the art language model which can be used for various natural language processing (NLP) tasks. \\\n",
    " My objective is to introduce BERT at a high level, and enable you to create practical applications using BERT. You need to have basic knowledge of Python as well as a basic idea of machine learning. \\\n",
    "After going through this post, you should be able to use BERT for Binary or multiclass classification or Regression model or Question Answering Application. \\\n",
    "BERT brings the transfer learning paradigm into the natural language processing area. Transfer learning means a model developed for a task can be reused as a starting point for another task. \\\n",
    "BERT is trained on the entirety of Wikipedia text (~2.5 billion words) as well as a book corpus (800 million words). You don’t need to repeat this compute intensive process to make practical use of BERT.\\\n",
    "For specific tasks such as classification or question answering you just need to do incremental training on a much smaller dataset. This process is called fine tuning. This is like getting a person who is proficient in English and providing them extra guidance on how to identify positive sentiment vs negative sentiment from movie reviews.\\\n",
    "This is a quick introduction on the BERT pre-training process. For practical uses, you will get a BERT pretrained model and you do not need to perform this step. \\\n",
    "BERT takes 2 chunks of text, which may include multiple sentences, as inputs (shown in the diagram above). These 2 chunks are referred to as Sentence 1 and Sentence 2 in the diagram above.  Before feeding sentences to BERT, 15% of words are masked. Also, sentence 2 may not always be in continuation of sentence 1. \\\n",
    "BERT’s pre-training process is like teaching the English language to the BERT model so that it can be used for various tasks which need knowledge of English. This is accomplished by 2 practice tasks given to BERT: \\\n",
    "1) Predict masked (hidden) tokens: To illustrate, words “favorite” and “to” are masked in the diagram. BERT will try to predict these masked tokens as part of pre-training. This is similar to the “fill in the blanks” task we may give to a student learning English. While trying to fill the missing words, the student will learn the language. This is referred to as Masked Language Model (MLM) in the natural language processing area. \\\n",
    "2) Next sentence prediction: Along with the technique mentioned above, BERT tries to predict if sentence 2 comes right after sentence 1 or not. This provides deeper understanding on sentence dependencies. \\\n",
    "To use BERT for practical use, we need to fine-tune it for specific tasks. This process finetunes the pre-trained model so that it can perform specific tasks such as text classification, sentiment analysis, question answering. In this process,BERT parameters get adjusted to do the specific task. '\n",
    "\n",
    "print('Passage:\\n', passage )\n",
    "\n",
    "print (f'Length of the passage: {len(passage.split())} words')\n",
    "\n",
    "\n",
    "question =\"What is full form of BERT\"\n",
    "print ('\\nQuestion 1:\\n', question)\n",
    "#Getting answer from BERT\n",
    "_, _ , _ , _, ans  = bert_answering_machine ( question, passage)\n",
    "print('\\nAnswer from BERT: ', ans ,  '\\n')\n",
    "\n",
    "question =\"What is author's purpose in writing this article \"\n",
    "print ('\\nQuestion 2:\\n', question)\n",
    "#Getting answer from BERT\n",
    "_, _ , _ , _, ans  = bert_answering_machine ( question, passage)\n",
    "print('\\nAnswer from BERT: ', ans ,  '\\n')\n",
    "\n",
    "question =\"What is full form of NLP \"\n",
    "print ('\\nQuestion 3:\\n', question)\n",
    "#Getting answer from BERT\n",
    "_, _ , _ , _, ans  = bert_answering_machine ( question, passage)\n",
    "print('\\nAnswer from BERT: ', ans ,  '\\n')\n",
    "\n",
    "question =\"What is Transfer Learning \"\n",
    "print ('\\nQuestion 4:\\n', question)\n",
    "#Getting answer from BERT\n",
    "_, _ , _ , _, ans  = bert_answering_machine ( question, passage)\n",
    "print('\\nAnswer from BERT: ', ans ,  '\\n')\n",
    "\n",
    "question =\"What is pre-training \"\n",
    "print ('\\nQuestion 5:\\n', question)\n",
    "#Getting answer from BERT\n",
    "_, _ , _ , _, ans  = bert_answering_machine ( question, passage)\n",
    "print('\\nAnswer from BERT: ', ans ,  '\\n')\n",
    "\n",
    "question =\"What corpus BERT was pre-trained \"\n",
    "print ('\\nQuestion 6:\\n', question)\n",
    "#Getting answer from BERT\n",
    "_, _ , _ , _, ans  = bert_answering_machine ( question, passage)\n",
    "print('\\nAnswer from BERT: ', ans ,  '\\n')\n",
    "\n",
    "question =\"Do we need to pre-train BERT model for general practical applications \"\n",
    "print ('\\nQuestion 7:\\n', question)\n",
    "#Getting answer from BERT\n",
    "_, _ , _ , _, ans  = bert_answering_machine ( question, passage)\n",
    "print('\\nAnswer from BERT: ', ans ,  '\\n')\n",
    "\n",
    "question =\"What is fine-tuning \"\n",
    "print ('\\nQuestion 8:\\n', question)\n",
    "#Getting answer from BERT\n",
    "_, _ , _ , _, ans  = bert_answering_machine ( question, passage)\n",
    "print('\\nAnswer from BERT: ', ans ,  '\\n')\n",
    "\n",
    "question =\"What is MLM \"\n",
    "print ('\\nQuestion 9:\\n', question)\n",
    "#Getting answer from BERT\n",
    "_, _ , _ , _, ans  = bert_answering_machine ( question, passage)\n",
    "print('\\nAnswer from BERT: ', ans ,  '\\n')\n",
    "\n",
    "question =\"Which words are masked in the diagram \"\n",
    "print ('\\nQuestion 10:\\n', question)\n",
    "#Getting answer from BERT\n",
    "_, _ , _ , _, ans  = bert_answering_machine ( question, passage)\n",
    "print('\\nAnswer from BERT: ', ans ,  '\\n')\n",
    "\n",
    "question =\"What perentage of words are hidden when feeding text to BERT for pre-trainining \"\n",
    "print ('\\nQuestion 11:\\n', question)\n",
    "#Getting answer from BERT\n",
    "_, _ , _ , _, ans  = bert_answering_machine ( question, passage)\n",
    "print('\\nAnswer from BERT: ', ans ,  '\\n')\n",
    "\n",
    "\n",
    "question =\"What is the analogy provided by the author, for masked language modelling used in BERT pre-training \"\n",
    "print ('\\nQuestion 12:\\n', question)\n",
    "#Getting answer from BERT\n",
    "_, _ , _ , _, ans  = bert_answering_machine ( question, passage)\n",
    "print('\\nAnswer from BERT: ', ans ,  '\\n')\n",
    "\n",
    "question =\"What is Random Forest alogorithm \"\n",
    "print ('\\nQuestion 13:\\n', question)\n",
    "#Getting answer from BERT\n",
    "_, _ , _ , _, ans  = bert_answering_machine ( question, passage)\n",
    "print('\\nAnswer from BERT: ', ans ,  '\\n')\n",
    "\n",
    "\n",
    "question =\"How this post may help you \"\n",
    "print ('\\nQuestion 14:\\n', question)\n",
    "#Getting answer from BERT\n",
    "_, _ , _ , _, ans  = bert_answering_machine ( question, passage)\n",
    "print('\\nAnswer from BERT: ', ans ,  '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
